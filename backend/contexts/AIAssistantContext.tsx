import React, { createContext, useContext, useState } from 'react';
import { useLanguage } from './LanguageContext';
import { useSoundDetection } from './SoundDetectionContext';

interface AIResponse {
  text: string;
  suggestions?: string[];
  actions?: Array<{
    label: string;
    action: () => void;
  }>;
}

interface AIAssistantContextType {
  generateResponse: (query: string) => Promise<AIResponse>;
  isProcessing: boolean;
}

const AIAssistantContext = createContext<AIAssistantContextType | undefined>(undefined);

export function AIAssistantProvider({ children }: { children: React.ReactNode }) {
  const { currentLanguage, t } = useLanguage();
  const { detections } = useSoundDetection();

  // Remove dependency on MLModelContext: use fixed defaults or derive from detections.
  // The app uses a single canonical ML model served by the backend (used for uploaded files).
  const modelSettings = {
    maxDuration: 3,
    sampleRate: 16000,
    sensitivity: detections.length > 0 ? (detections[0].confidence ?? 0.75) : 0.75,
    confidenceThreshold: 0.6,
    batchSize: 1024,
    enablePreprocessing: true,
    enablePostprocessing: true,
  } as const;

  const modelPerformance = {
    accuracy: 0.71,
    inferenceTime: 50,
    precision: 0.8,
    f1Score: 0.8,
  } as const;
  
  // Canonical class names (match contexts/pred_with_audio.py)
  const class_names = [
    "applause_no_speech", "applause_speech",
    "cat_meowing_no_speech", "cat_meowing_speech",
    "cough_no_speech", "cough_speech",
    "crying_no_speech", "crying_speech",
    "dishes_pot_pan_no_speech", "dishes_pot_pan_speech",
    "dog_barking_no_speech", "dog_barking_speech",
    "doorbell_no_speech", "doorbell_speech",
    "drill_no_speech", "drill_speech",
    "glass_breaking_no_speech", "glass_breaking_speech",
    "gun_shot_no_speech", "gun_shot_speech",
    "slam_no_speech", "slam_speech",
    "toilet_flush_no_speech", "toilet_flush_speech"
  ];
  const [isProcessing, setIsProcessing] = useState(false);

  // sanitize assistant text: remove emojis and markdown markers but preserve plain text and line breaks
  const sanitizeText = (t: string) => {
    if (!t || typeof t !== 'string') return t;
    let s = t;
    // remove common markdown emphasis markers but keep the text
    s = s.replace(/\*\*|__|`/g, '');

    // replace common bullet characters with a newline so lists remain readable
    s = s.replace(/[тАвтАв┬╖]/g, '\n');
    s = s.replace(/[тЖТтЮбтЖР]/g, ' -> ');

    // remove a broad set of emoji/pictograph ranges while preserving normal punctuation and letters
    try {
      // Unicode property escape (works in modern JS runtimes)
      s = s.replace(/\p{Extended_Pictographic}/gu, '');
    } catch (e) {
      // fallback explicit ranges if Unicode property escapes unsupported
      s = s.replace(/[\u{1F300}-\u{1F5FF}\u{1F600}-\u{1F64F}\u{1F680}-\u{1F6FF}\u2600-\u26FF}\u2700-\u27BF]/gu, '');
    }

    // Trim trailing spaces on each line but preserve newlines
    s = s.split('\n').map(line => line.replace(/\s+/g, ' ').trim()).join('\n');
    // collapse multiple blank lines
    s = s.replace(/\n{3,}/g, '\n\n').trim();
    return s;
  };

  const getLatest = () => detections && detections.length > 0 ? detections[0] : null;

  const friendlyLabel = (raw: string) => {
    // convert class names like 'cough_speech' -> 'cough'
    if (!raw) return raw;
    const parts = raw.split('_');
    return parts[0];
  };

  const knowledgeBase = {
    en: {
      patterns: {
        'how.*work|working|function': {
          response: `SoundAware uses advanced TensorFlow Lite machine learning models to analyze audio patterns in real-time. Here's how it works:

ЁЯза **AI Processing**: The app processes audio using a neural network trained on 25+ household sounds
ЁЯФК **Real-time Analysis**: Audio is analyzed in chunks of ${modelSettings.maxDuration} seconds with ${modelSettings.sampleRate}Hz sample rate
ЁЯУК **Confidence Scoring**: Each detection gets a confidence score (currently set to ${Math.round(modelSettings.confidenceThreshold * 100)}% minimum)
ЁЯФТ **Local Processing**: All analysis happens on your device - no data leaves your phone

The model achieves ${Math.round(modelPerformance.accuracy * 100)}% accuracy with ${modelPerformance.inferenceTime}ms inference time.`,
          suggestions: ['What sounds can it detect?', 'How to improve accuracy?', 'Privacy and security?']
        },
        'sound.*detect|what.*sound|which.*sound': {
          response: (ctx: { latest: any }) => {
            const latest = ctx.latest;
            const latestText = latest ? `Latest: ${friendlyLabel(latest.soundType)} (${Math.round((latest.confidence||0)*100)}%)` : 'No recent detections';
            return `SoundAware detects ${class_names.length} household sounds across categories. ${latestText}. Use the Record tab to capture new audio or upload files for analysis.`;
          },
          suggestions: ['How accurate is detection?', 'Can I upload audio files?', 'Real-time monitoring?']
        },
        'which.*classes|class list|classes': {
          response: () => `The model recognizes the following classes: ${class_names.join(', ')}.`,
          suggestions: ['What is the accuracy?', 'How to improve accuracy?']
        },
        'accura.*|precision|reliable': {
          response: `SoundAware's accuracy depends on several factors:

ЁЯУИ **Overall Performance**:
- Model Accuracy: ${Math.round(modelPerformance.accuracy * 100)}%
- Precision: ${Math.round(modelPerformance.precision * 100)}%
- F1 Score: ${Math.round(modelPerformance.f1Score * 100)}%
- Average Inference Time: ${modelPerformance.inferenceTime}ms

ЁЯОп **Confidence Levels**:
- 90-100%: Extremely reliable
- 80-89%: Highly reliable  
- 70-79%: Good reliability
- 60-69%: Moderate reliability
- Below 60%: May need verification

ЁЯФз **Current Settings**:
- Sensitivity: ${Math.round(modelSettings.sensitivity * 100)}%
- Confidence Threshold: ${Math.round(modelSettings.confidenceThreshold * 100)}%
- Preprocessing: ${modelSettings.enablePreprocessing ? 'Enabled' : 'Disabled'}

**Tips to improve accuracy**: Record in quiet environments, hold device close to sound source, ensure 3-10 second recordings.`,
          suggestions: ['How to improve accuracy?', 'What affects detection?', 'Model settings?']
        },
        'privacy|security|safe|data': {
          response: `SoundAware prioritizes your privacy and security:

ЁЯФТ **Complete Privacy**:
- All audio processing happens locally on your device
- No audio data is ever sent to external servers
- No cloud storage or remote analysis
- Your recordings stay on your device only

ЁЯЫбя╕П **Security Features**:
- Local TensorFlow Lite model execution
- No internet connection required for detection
- Encrypted local storage for settings and history
- No user accounts or data collection

ЁЯУ▒ **Data Control**:
- You can delete detection history anytime
- Audio recordings are stored temporarily and can be removed
- Export your data as CSV for backup
- Complete control over your information

**Your audio never leaves your device - guaranteed!**`,
          suggestions: ['How does local processing work?', 'Can I export my data?', 'Storage management?']
        },
        'upload.*file|file.*upload|import.*audio': {
          response: (ctx: { latest: any }) => {
            const latest = ctx.latest;
            const note = latest ? `Last upload detected: ${friendlyLabel(latest.soundType)} (${Math.round((latest.confidence||0)*100)}%)` : '';
            return `Yes тАФ you can upload audio files (MP3, WAV, M4A, AAC, FLAC). Max 50MB. Recommended duration ${modelSettings.maxDuration}s. ${note}`;
          },
          suggestions: ['What file formats work best?', 'File size limits?', 'Processing time?']
        },
        'real.*time|live|instant': {
          response: `SoundAware provides true real-time audio analysis:

тЪб **Performance Metrics**:
- Detection Latency: ${modelPerformance.inferenceTime}ms
- Processing Speed: Real-time (no delays)
- Buffer Size: ${modelSettings.batchSize} samples
- Sample Rate: ${modelSettings.sampleRate}Hz

ЁЯФД **Real-time Features**:
- Continuous audio monitoring
- Instant sound classification
- Live audio visualization
- Immediate notifications for important sounds
- Background processing capability

тЪЩя╕П **Optimization**:
- Efficient ML model (${Math.round(modelPerformance.accuracy * 100)}% accuracy)
- Battery-optimized processing
- Minimal CPU usage
- Smart audio buffering

The app can detect and classify sounds within 1-2 seconds of occurrence!`,
          suggestions: ['Battery usage?', 'Background monitoring?', 'Performance optimization?']
        },
        // direct yes/no queries like "is this a cough?" or "is that dog barking?"
        'is\s+.*(cough|dog|cat|applause|doorbell|glass|gun|drill|slam|toilet|dishes|crying|bark|meow)': {
          response: (ctx: { latest: any; query?: string }) => {
            const latest = ctx.latest;
            const q = (ctx.query || '').toLowerCase();
            if (!latest) return 'I have no recent detection to compare. Please record or upload a short clip and try again.';
            // find which keyword user asked about
            const keywords = ['cough','dog','cat','applause','doorbell','glass','gun','drill','slam','toilet','dishes','crying','bark','meow'];
            const asked = keywords.find(k => q.includes(k));
            if (!asked) return `I couldn't identify which sound you're asking about.`;
            // map latest.soundType to friendly
            const latestFriendly = friendlyLabel(latest.soundType).toLowerCase();
            if (latestFriendly.includes(asked) || asked.includes(latestFriendly)) {
              return `Yes тАФ recent detection: ${friendlyLabel(latest.soundType)} with ${(Math.round((latest.confidence||0)*100))}% confidence.`;
            }
            return `No тАФ the most recent detection was ${friendlyLabel(latest.soundType)} (${Math.round((latest.confidence||0)*100)}% confidence), not ${asked}.`;
          }
        },
        'help|support|how.*use|tutorial': {
          response: `Welcome to SoundAware! Here's your complete guide:

ЁЯУ▒ **Main Features**:

1я╕ПтГг **Home Tab**: Overview of recent detections and quick stats
2я╕ПтГг **Record Tab**: Live recording and file upload for analysis  
3я╕ПтГг **History Tab**: View all past detections, export CSV, share summaries
4я╕ПтГг **Notifications**: Alerts for important sound detections
5я╕ПтГг **AI Chat**: Ask questions about the app (you're here!)
6я╕ПтГг **Settings**: Customize sensitivity, language, and model settings

ЁЯОп **Quick Start**:
- Tap Record tab тЖТ Tap microphone тЖТ Let it listen for 3-10 seconds
- Or upload an audio file for instant analysis
- Check History for all your detections
- Adjust sensitivity in Settings for better results

ЁЯУК **Current Status**: ${detections.length} detections recorded, ${Math.round(modelSettings.sensitivity * 100)}% sensitivity`,
          suggestions: ['Recording tips?', 'Best practices?', 'Troubleshooting?']
        },
        'battery|power|consumption': {
          response: `SoundAware is optimized for minimal battery usage:

ЁЯФЛ **Battery Optimization**:
- Efficient TensorFlow Lite model (only ${modelPerformance.inferenceTime}ms per inference)
- Smart audio buffering to reduce CPU usage
- Background processing optimization
- Automatic sleep mode when inactive

тЪЩя╕П **Power Management**:
- Audio preprocessing: ${modelSettings.enablePreprocessing ? 'Enabled (slight battery impact)' : 'Disabled (battery optimized)'}
- Batch processing: ${modelSettings.batchSize} samples (optimized for efficiency)
- Sample rate: ${modelSettings.sampleRate}Hz (balanced quality/power)

ЁЯТб **Battery Tips**:
- Use lower sensitivity for longer battery life
- Disable preprocessing if not needed
- Enable auto-recording only when necessary
- Close app when not actively monitoring

Typical usage: 2-5% battery per hour of continuous monitoring.`,
          suggestions: ['Power saving tips?', 'Background usage?', 'Optimization settings?']
        },
        'improve.*accuracy|better.*detection|enhance': {
          response: `Here are proven ways to improve detection accuracy:

ЁЯОп **Recording Best Practices**:
- Hold device 1-3 feet from sound source
- Record in quiet environment (minimize background noise)
- Ensure 3-10 second recordings for best results
- Avoid covering microphone

тЪЩя╕П **Optimal Settings** (Current vs Recommended):
- Sensitivity: ${Math.round(modelSettings.sensitivity * 100)}% (try 70-80% for balanced results)
- Confidence Threshold: ${Math.round(modelSettings.confidenceThreshold * 100)}% (60-70% recommended)
- Preprocessing: ${modelSettings.enablePreprocessing ? 'тЬЕ Enabled' : 'тЭМ Disabled'} (enable for better accuracy)
- Postprocessing: ${modelSettings.enablePostprocessing ? 'тЬЕ Enabled' : 'тЭМ Disabled'} (enable for cleaner results)

ЁЯФз **Advanced Optimization**:
- Use higher sample rate (44.1kHz) for complex sounds
- Enable both preprocessing and postprocessing
- Adjust batch size based on device performance

Would you like me to suggest optimal settings for your use case?`,
          suggestions: ['Optimal settings for my device?', 'Troubleshoot poor detection?', 'Advanced configuration?']
        },
        'language|translate|multilingual': {
          response: `SoundAware supports 8 languages with full UI translation:

ЁЯМН **Available Languages**:
- English (English)
- рд╣рд┐рдВрджреА (Hindi) 
- рикрй░риЬри╛римрйА (Punjabi)
- ркЧрлБркЬрк░рк╛ркдрлА (Gujarati)
- родрооро┐ро┤рпН (Tamil)
- р░др▒Жр░▓р▒Бр░Чр▒Б (Telugu)
- ржмрж╛ржВрж▓рж╛ (Bengali)
- рдорд░рд╛рдареА (Marathi)

ЁЯФД **Language Features**:
- Complete UI translation
- Voice assistant in your language
- AI responses in selected language
- Localized date/time formats
- Cultural sound preferences

ЁЯУ▒ **How to Change Language**:
1. Go to Settings tab
2. Tap on Language section
3. Select your preferred language
4. App will restart with new language

Current language: ${currentLanguage === 'en' ? 'English' : currentLanguage === 'hi' ? 'рд╣рд┐рдВрджреА' : currentLanguage === 'pa' ? 'рикрй░риЬри╛римрйА' : currentLanguage}`,
          suggestions: ['Voice commands in my language?', 'Add new language?', 'Translation accuracy?']
        },
        'export|csv|download|backup': {
          response: `SoundAware offers comprehensive data export options:

ЁЯУК **CSV Export Features**:
- Complete detection history with timestamps
- Confidence scores and sound types
- Duration and metadata for each detection
- Automatic filename with date
- Compatible with Excel, Google Sheets

ЁЯУ▒ **Export Process**:
1. Go to History tab
2. Tap "Export CSV" button
3. File downloads automatically (web) or opens share dialog (mobile)
4. Data includes: Date, Time, Sound Type, Confidence %, Duration

ЁЯУд **Share Options**:
- Quick summary with recent detections
- Statistical overview
- Formatted for messaging apps
- Email-friendly format

ЁЯУИ **Your Current Data**:
- Total detections: ${detections.length}
- Average confidence: ${detections.length > 0 ? Math.round(detections.reduce((sum, d) => sum + d.confidence, 0) / detections.length * 100) : 0}%
- Date range: ${detections.length > 0 ? `${detections[detections.length - 1]?.timestamp.toLocaleDateString()} to ${detections[0]?.timestamp.toLocaleDateString()}` : 'No data yet'}`,
          suggestions: ['How to backup data?', 'Share with others?', 'Data formats?']
        }
      },
      fallback: `I'm your AI assistant for SoundAware! I can help you with:

ЁЯОп **App Features**: Recording, detection history, settings, notifications
ЁЯФз **Technical Support**: Model configuration, accuracy optimization, troubleshooting  
ЁЯУК **Data Management**: Export options, sharing, privacy settings
ЁЯМН **Languages**: Multi-language support and voice commands
ЁЯФТ **Privacy**: Local processing, security features, data control

**Current App Status**:
- Detections: ${detections.length} total
- Model accuracy: ${Math.round(modelPerformance.accuracy * 100)}%
- Language: ${currentLanguage}
- Sensitivity: ${Math.round(modelSettings.sensitivity * 100)}%

What would you like to know more about?`
    },
    hi: {
      patterns: {
        'рдХреИрд╕реЗ.*рдХрд╛рдо|рдХрд╛рд░реНрдп.*рдХреИрд╕реЗ|рдлрдВрдХреНрд╢рди': {
          response: `SoundAware рдЙрдиреНрдирдд TensorFlow Lite рдорд╢реАрди рд▓рд░реНрдирд┐рдВрдЧ рдореЙрдбрд▓ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ рд░рд┐рдпрд▓-рдЯрд╛рдЗрдо рдореЗрдВ рдСрдбрд┐рдпреЛ рдкреИрдЯрд░реНрди рдХрд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдХрд░рддрд╛ рд╣реИ:

ЁЯза **AI рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг**: рдРрдк 25+ рдШрд░реЗрд▓реВ рдзреНрд╡рдирд┐рдпреЛрдВ рдкрд░ рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдиреНрдпреВрд░рд▓ рдиреЗрдЯрд╡рд░реНрдХ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИ
ЁЯФК **рд░рд┐рдпрд▓-рдЯрд╛рдЗрдо рд╡рд┐рд╢реНрд▓реЗрд╖рдг**: ${modelSettings.maxDuration} рд╕реЗрдХрдВрдб рдХреЗ рдЪрдВрдХ рдореЗрдВ ${modelSettings.sampleRate}Hz рд╕реИрдВрдкрд▓ рд░реЗрдЯ рдХреЗ рд╕рд╛рде
ЁЯУК **рд╡рд┐рд╢реНрд╡рд╛рд╕ рд╕реНрдХреЛрд░рд┐рдВрдЧ**: рдкреНрд░рддреНрдпреЗрдХ рдкрд╣рдЪрд╛рди рдХреЛ рд╡рд┐рд╢реНрд╡рд╛рд╕ рд╕реНрдХреЛрд░ рдорд┐рд▓рддрд╛ рд╣реИ (рд╡рд░реНрддрдорд╛рди рдореЗрдВ ${Math.round(modelSettings.confidenceThreshold * 100)}% рдиреНрдпреВрдирддрдо)
ЁЯФТ **рд╕реНрдерд╛рдиреАрдп рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг**: рд╕рднреА рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдЖрдкрдХреЗ рдбрд┐рд╡рд╛рдЗрд╕ рдкрд░ рд╣реЛрддрд╛ рд╣реИ

рдореЙрдбрд▓ ${Math.round(modelPerformance.accuracy * 100)}% рд╕рдЯреАрдХрддрд╛ рдХреЗ рд╕рд╛рде ${modelPerformance.inferenceTime}ms рдореЗрдВ рдкрд░рд┐рдгрд╛рдо рджреЗрддрд╛ рд╣реИред`,
          suggestions: ['рдХреМрди рд╕реА рдЖрд╡рд╛рдЬреЗрдВ рдкрд╣рдЪрд╛рди рд╕рдХрддрд╛ рд╣реИ?', 'рд╕рдЯреАрдХрддрд╛ рдХреИрд╕реЗ рдмрдврд╝рд╛рдПрдВ?', 'рдЧреЛрдкрдиреАрдпрддрд╛ рдФрд░ рд╕реБрд░рдХреНрд╖рд╛?']
        },
        'рдЖрд╡рд╛рдЬ.*рдкрд╣рдЪрд╛рди|рдзреНрд╡рдирд┐.*рдкрддрд╛|рдХреМрди.*рдЖрд╡рд╛рдЬ': {
          response: `SoundAware 25+ рдШрд░реЗрд▓реВ рдзреНрд╡рдирд┐рдпреЛрдВ рдХреА рдкрд╣рдЪрд╛рди рдХрд░ рд╕рдХрддрд╛ рд╣реИ:

ЁЯПа **рд░рд╕реЛрдИ рдХреА рдЖрд╡рд╛рдЬреЗрдВ**: рдорд╛рдЗрдХреНрд░реЛрд╡реЗрд╡ рдмреАрдк, рдХрд┐рдЪрди рдЯрд╛рдЗрдорд░, рдЙрдмрд▓рддрд╛ рдкрд╛рдиреА, рдмреНрд▓реЗрдВрдбрд░, рдХреЙрдлреА рдореЗрдХрд░
ЁЯФФ **рд╕реБрд░рдХреНрд╖рд╛ рдЕрд▓рд░реНрдЯ**: рдбреЛрд░рдмреЗрд▓, рджрд░рд╡рд╛рдЬрд╛ рдЦрдЯрдЦрдЯрд╛рдирд╛, рдЦрд┐рдбрд╝рдХреА рдЯреВрдЯрдирд╛, рдХрд╛рд░ рдЕрд▓рд╛рд░реНрдо
ЁЯПа **рдЙрдкрдХрд░рдг**: рд╡рд╛рд╢рд┐рдВрдЧ рдорд╢реАрди, рд╡реИрдХреНрдпреВрдо рдХреНрд▓реАрдирд░, рдПрдпрд░ рдХрдВрдбреАрд╢рдирд░, рдбреНрд░рд╛рдпрд░
ЁЯРХ **рдкрд╛рд▓рддреВ рдЬрд╛рдирд╡рд░**: рдХреБрддреНрддреЗ рдХрд╛ рднреМрдВрдХрдирд╛, рдмрд┐рд▓реНрд▓реА рдХрд╛ рдореНрдпрд╛рдКрдВ, рдкрдХреНрд╖рд┐рдпреЛрдВ рдХрд╛ рдЪрд╣рдЪрд╣рд╛рдирд╛
ЁЯЪи **рдЖрдкрд╛рддрдХрд╛рд▓**: рд╕реНрдореЛрдХ рдЕрд▓рд╛рд░реНрдо, рдХрд╛рд░реНрдмрди рдореЛрдиреЛрдСрдХреНрд╕рд╛рдЗрдб рдЕрд▓рд╛рд░реНрдо, рдлрд╛рдпрд░ рдЕрд▓рд╛рд░реНрдо
ЁЯУ▒ **рд╕рдВрдЪрд╛рд░**: рдлреЛрди рд░рд┐рдВрдЧ, рдореИрд╕реЗрдЬ рдиреЛрдЯрд┐рдлрд┐рдХреЗрд╢рди, рд╡реАрдбрд┐рдпреЛ рдХреЙрд▓

рд╡рд░реНрддрдорд╛рди рдЖрдВрдХрдбрд╝реЗ: ${detections.length} рдХреБрд▓ рдкрд╣рдЪрд╛рди, ${detections.length > 0 ? Math.round(detections.reduce((sum, d) => sum + d.confidence, 0) / detections.length * 100) : 0}% рдФрд╕рдд рд╡рд┐рд╢реНрд╡рд╛рд╕ред`,
          suggestions: ['рд╕рдЯреАрдХрддрд╛ рдХреИрд╕реА рд╣реИ?', 'рдлрд╛рдЗрд▓ рдЕрдкрд▓реЛрдб рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ?', 'рд░рд┐рдпрд▓-рдЯрд╛рдЗрдо рдореЙрдирд┐рдЯрд░рд┐рдВрдЧ?']
        },
        'рд╕рдЯреАрдХрддрд╛|рдкрд░рд┐рд╢реБрджреНрдзрддрд╛|рд╡рд┐рд╢реНрд╡рд╕рдиреАрдп': {
          response: `SoundAware рдХреА рд╕рдЯреАрдХрддрд╛ рдХрдИ рдХрд╛рд░рдХреЛрдВ рдкрд░ рдирд┐рд░реНрднрд░ рдХрд░рддреА рд╣реИ:

ЁЯУИ **рд╕рдордЧреНрд░ рдкреНрд░рджрд░реНрд╢рди**:
- рдореЙрдбрд▓ рд╕рдЯреАрдХрддрд╛: ${Math.round(modelPerformance.accuracy * 100)}%
- рдкрд░рд┐рд╢реБрджреНрдзрддрд╛: ${Math.round(modelPerformance.precision * 100)}%
- F1 рд╕реНрдХреЛрд░: ${Math.round(modelPerformance.f1Score * 100)}%
- рдФрд╕рдд рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рд╕рдордп: ${modelPerformance.inferenceTime}ms

ЁЯОп **рд╡рд┐рд╢реНрд╡рд╛рд╕ рд╕реНрддрд░**:
- 90-100%: рдЕрддреНрдпрдзрд┐рдХ рд╡рд┐рд╢реНрд╡рд╕рдиреАрдп
- 80-89%: рдЕрддреНрдпрдзрд┐рдХ рд╡рд┐рд╢реНрд╡рд╕рдиреАрдп
- 70-79%: рдЕрдЪреНрдЫреА рд╡рд┐рд╢реНрд╡рд╕рдиреАрдпрддрд╛
- 60-69%: рдордзреНрдпрдо рд╡рд┐рд╢реНрд╡рд╕рдиреАрдпрддрд╛

ЁЯФз **рд╡рд░реНрддрдорд╛рди рд╕реЗрдЯрд┐рдВрдЧреНрд╕**:
- рд╕рдВрд╡реЗрджрдирд╢реАрд▓рддрд╛: ${Math.round(modelSettings.sensitivity * 100)}%
- рд╡рд┐рд╢реНрд╡рд╛рд╕ рд╕реАрдорд╛: ${Math.round(modelSettings.confidenceThreshold * 100)}%

**рд╕рдЯреАрдХрддрд╛ рдмрдврд╝рд╛рдиреЗ рдХреЗ рдЯрд┐рдкреНрд╕**: рд╢рд╛рдВрдд рд╡рд╛рддрд╛рд╡рд░рдг рдореЗрдВ рд░рд┐рдХреЙрд░реНрдб рдХрд░реЗрдВ, рдбрд┐рд╡рд╛рдЗрд╕ рдХреЛ рдзреНрд╡рдирд┐ рд╕реНрд░реЛрдд рдХреЗ рдкрд╛рд╕ рд░рдЦреЗрдВред`,
          suggestions: ['рд╕рдЯреАрдХрддрд╛ рдХреИрд╕реЗ рдмрдврд╝рд╛рдПрдВ?', 'рд╕реЗрдЯрд┐рдВрдЧреНрд╕ рдХреИрд╕реЗ рдмрджрд▓реЗрдВ?', 'рдмреЗрд╣рддрд░ рдкрд░рд┐рдгрд╛рдо рдХреИрд╕реЗ рдкрд╛рдПрдВ?']
        },
        'classes|рдХреНрд▓рд╛рд╕|рдХреМрди рд╕реА рдХреНрд▓рд╛рд╕': {
          response: `рдореЙрдбрд▓ рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рдХреНрд▓рд╛рд╕реЗрд╕ рдкрд╣рдЪрд╛рдирддрд╛ рд╣реИ: ${class_names.join(', ')}`,
          suggestions: ['рдореЙрдбрд▓ рдХреА рд╕рдЯреАрдХрддрд╛ рдХреНрдпрд╛ рд╣реИ?', 'рд╕рдЯреАрдХрддрд╛ рдХреИрд╕реЗ рдмрдврд╝рд╛рдПрдВ?']
        }
      },
      fallback: `рдореИрдВ SoundAware рдХрд╛ AI рд╕рд╣рд╛рдпрдХ рд╣реВрдВ! рдореИрдВ рдЖрдкрдХреА рдорджрдж рдХрд░ рд╕рдХрддрд╛ рд╣реВрдВ:

ЁЯОп **рдРрдк рд╕реБрд╡рд┐рдзрд╛рдПрдВ**: рд░рд┐рдХреЙрд░реНрдбрд┐рдВрдЧ, рдкрд╣рдЪрд╛рди рдЗрддрд┐рд╣рд╛рд╕, рд╕реЗрдЯрд┐рдВрдЧреНрд╕, рд╕реВрдЪрдирд╛рдПрдВ
ЁЯФз **рддрдХрдиреАрдХреА рд╕рд╣рд╛рдпрддрд╛**: рдореЙрдбрд▓ рдХреЙрдиреНрдлрд╝рд┐рдЧрд░реЗрд╢рди, рд╕рдЯреАрдХрддрд╛ рдЕрдиреБрдХреВрд▓рди
ЁЯУК **рдбреЗрдЯрд╛ рдкреНрд░рдмрдВрдзрди**: рдирд┐рд░реНрдпрд╛рдд рд╡рд┐рдХрд▓реНрдк, рд╕рд╛рдЭрд╛рдХрд░рдг, рдЧреЛрдкрдиреАрдпрддрд╛
ЁЯМН **рднрд╛рд╖рд╛рдПрдВ**: рдмрд╣реБрднрд╛рд╖реА рд╕рдорд░реНрдерди рдФрд░ рдЖрд╡рд╛рдЬ рдХрдорд╛рдВрдб

**рд╡рд░реНрддрдорд╛рди рдРрдк рд╕реНрдерд┐рддрд┐**:
- рдкрд╣рдЪрд╛рди: ${detections.length} рдХреБрд▓
- рдореЙрдбрд▓ рд╕рдЯреАрдХрддрд╛: ${Math.round(modelPerformance.accuracy * 100)}%
- рд╕рдВрд╡реЗрджрдирд╢реАрд▓рддрд╛: ${Math.round(modelSettings.sensitivity * 100)}%

рдЖрдк рдХреНрдпрд╛ рдЬрд╛рдирдирд╛ рдЪрд╛рд╣рддреЗ рд╣реИрдВ?`
    }
  };

  const generateResponse = async (query: string): Promise<AIResponse> => {
    setIsProcessing(true);
    
    // Simulate processing time
    await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 1500));
    
    try {
      const language = knowledgeBase[currentLanguage as keyof typeof knowledgeBase] || knowledgeBase.en;
      const queryLower = query.toLowerCase();
      
      // Find matching pattern
      for (const [pattern, response] of Object.entries(language.patterns)) {
        const regex = new RegExp(pattern, 'i');
        if (regex.test(queryLower)) {
          const latest = getLatest();
          let text: string;
          if (typeof response.response === 'function') {
            try {
              // call dynamic response function with context (cast to any to avoid strict typing issues)
              text = (response.response as any)({ latest, query: queryLower, class_names });
            } catch (e) {
              text = language.fallback;
            }
          } else {
            text = response.response as string;
          }
          setIsProcessing(false);
          return {
            text: sanitizeText(text),
            suggestions: (response as any).suggestions ?? []
          };
        }
      }
      
      // Fallback response
      setIsProcessing(false);
      return {
        text: sanitizeText(language.fallback),
        suggestions: ['How does it work?', 'What sounds can it detect?', 'Help and support?']
      };
    } catch (error) {
      setIsProcessing(false);
      const errMsg = currentLanguage === 'hi' 
        ? 'рдХреНрд╖рдорд╛ рдХрд░реЗрдВ, рдореБрдЭреЗ рдЖрдкрдХрд╛ рдкреНрд░рд╢реНрди рд╕рдордЭрдиреЗ рдореЗрдВ рдХрдард┐рдирд╛рдИ рд╣реЛ рд░рд╣реА рд╣реИред рдХреГрдкрдпрд╛ рджреВрд╕рд░реЗ рддрд░реАрдХреЗ рд╕реЗ рдкреВрдЫреЗрдВред'
        : 'I apologize, but I\'m having trouble understanding your question. Could you please rephrase it?';
      return {
        text: sanitizeText(errMsg),
        suggestions: ['How does it work?', 'What sounds can it detect?', 'Help and support?']
      };
    }
  };

  return (
    <AIAssistantContext.Provider value={{
      generateResponse,
      isProcessing,
    }}>
      {children}
    </AIAssistantContext.Provider>
  );
}

export function useAIAssistant() {
  const context = useContext(AIAssistantContext);
  if (context === undefined) {
    throw new Error('useAIAssistant must be used within a AIAssistantProvider');
  }
  return context;
}